{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom scipy import spatial\nfrom collections import Counter\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n \nds_path = \"/kaggle/input/complete-poetryfoundationorg-dataset/\"\nglove_path = \"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.%dd.txt\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-04T20:18:19.917803Z","iopub.execute_input":"2022-01-04T20:18:19.918082Z","iopub.status.idle":"2022-01-04T20:18:20.650757Z","shell.execute_reply.started":"2022-01-04T20:18:19.918033Z","shell.execute_reply":"2022-01-04T20:18:20.649953Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import keras.backend as K\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Embedding, Input, LSTM, GRU\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:20.652723Z","iopub.execute_input":"2022-01-04T20:18:20.653000Z","iopub.status.idle":"2022-01-04T20:18:25.114685Z","shell.execute_reply.started":"2022-01-04T20:18:20.652954Z","shell.execute_reply":"2022-01-04T20:18:25.113945Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"poems_df = pd.read_csv(os.path.join(ds_path, \"kaggle_poem_dataset.csv\"))\npoems_df.head(10)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-01-04T20:18:25.115975Z","iopub.execute_input":"2022-01-04T20:18:25.116283Z","iopub.status.idle":"2022-01-04T20:18:25.662839Z","shell.execute_reply.started":"2022-01-04T20:18:25.116228Z","shell.execute_reply":"2022-01-04T20:18:25.662003Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"poems_df.groupby(\"Author\").agg({\"Content\": \"count\"}).sort_values(\"Content\", ascending=False).head(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:25.664404Z","iopub.execute_input":"2022-01-04T20:18:25.664706Z","iopub.status.idle":"2022-01-04T20:18:25.688555Z","shell.execute_reply.started":"2022-01-04T20:18:25.664662Z","shell.execute_reply":"2022-01-04T20:18:25.687839Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"william_poems = poems_df[poems_df[\"Author\"] == \"William Shakespeare\"]\nprint(\"Some of the lines are: \")\nprint(william_poems.iloc[0, 4].split('\\n')[:4])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:25.691384Z","iopub.execute_input":"2022-01-04T20:18:25.691799Z","iopub.status.idle":"2022-01-04T20:18:25.701444Z","shell.execute_reply.started":"2022-01-04T20:18:25.691750Z","shell.execute_reply":"2022-01-04T20:18:25.700787Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"poems_combined = \"\\n\".join(william_poems.iloc[:, 4].values)\nprint(\"Total number of characters: \", len(poems_combined))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:25.704001Z","iopub.execute_input":"2022-01-04T20:18:25.704248Z","iopub.status.idle":"2022-01-04T20:18:25.712905Z","shell.execute_reply.started":"2022-01-04T20:18:25.704200Z","shell.execute_reply":"2022-01-04T20:18:25.712255Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"poem_lines = poems_combined.split('\\n')\nprint(\"Number of lines in the dataset: \", len(poem_lines))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:25.714831Z","iopub.execute_input":"2022-01-04T20:18:25.715053Z","iopub.status.idle":"2022-01-04T20:18:25.723039Z","shell.execute_reply.started":"2022-01-04T20:18:25.715013Z","shell.execute_reply":"2022-01-04T20:18:25.722129Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"input_lines = [\"<sos> \"+line for line in poem_lines] # in each of the input we add <sos> token idicating the begining of a line\ntarget_lines = [line+ \" <eos>\" for line in poem_lines] # while target lines are appended with with <eos> token indicating end of the line","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:25.724504Z","iopub.execute_input":"2022-01-04T20:18:25.724924Z","iopub.status.idle":"2022-01-04T20:18:25.731789Z","shell.execute_reply.started":"2022-01-04T20:18:25.724774Z","shell.execute_reply":"2022-01-04T20:18:25.731038Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_lines = map(str.split, input_lines)\nlen_of_lines = map(len, tokenized_lines)\nlen_frequencies = Counter(list(len_of_lines))\n\nsorted(len_frequencies.items())","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:25.733335Z","iopub.execute_input":"2022-01-04T20:18:25.733929Z","iopub.status.idle":"2022-01-04T20:18:25.745968Z","shell.execute_reply.started":"2022-01-04T20:18:25.733700Z","shell.execute_reply":"2022-01-04T20:18:25.745270Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 500 \nBATCH_SIZE = 64 \nLATENT_DIM = 200 \nEMBEDDING_DIM = 200 \nMAX_VOCAB_SIZE = 30000 \nVALIDATION_SPLIT = 0.2","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:25.747720Z","iopub.execute_input":"2022-01-04T20:18:25.748326Z","iopub.status.idle":"2022-01-04T20:18:25.757153Z","shell.execute_reply.started":"2022-01-04T20:18:25.747966Z","shell.execute_reply":"2022-01-04T20:18:25.756589Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class SequenceGenerator():\n    \n    def __init__(self, input_lines, target_lines, max_seq_len=None, max_vocab_size=10000, embedding_dim=200):\n    \n        self.input_lines = input_lines\n        self.target_lines = target_lines\n        \n        self.MAX_SEQ_LEN = max_seq_len\n        self.MAX_VOCAB_SIZE = max_vocab_size\n        self.EMBEDDING_DIM = embedding_dim\n    \n    \n    def initialize_embeddings(self):\n       \n        self.word2vec = {}\n        with open(glove_path%self.EMBEDDING_DIM, 'r') as file:\n            for line in file:\n                vectors = line.split()\n                self.word2vec[vectors[0]] = np.asarray(vectors[1:], dtype=\"float32\")\n\n        self.num_words = min(self.MAX_VOCAB_SIZE, len(self.word2idx)+1)\n        self.embeddings_matrix = np.zeros((self.num_words, self.EMBEDDING_DIM))\n        \n        for word, idx in self.word2idx.items():\n            if idx <= self.num_words:\n                word_embeddings = self.word2vec.get(word)\n                if word_embeddings is not None:\n                    self.embeddings_matrix[idx] = word_embeddings\n                    \n        self.idx2word = {v:k for k,v in self.word2idx.items()}\n    \n    \n    def prepare_sequences(self, filters=''):\n    \n        self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, filters='')\n        self.tokenizer.fit_on_texts(self.input_lines+self.target_lines)\n        self.word2idx = self.tokenizer.word_index\n        self.initialize_embeddings()\n\n        self.input_sequences = self.tokenizer.texts_to_sequences(self.input_lines)\n        self.target_sequences = self.tokenizer.texts_to_sequences(self.target_lines)\n        \n        \n        max_seq_len = max(list(map(len, self.input_lines+self.target_lines)))\n        if self.MAX_SEQ_LEN:\n            self.MAX_SEQ_LEN = min(self.MAX_SEQ_LEN, max_seq_len)\n        else:\n            self.MAX_SEQ_LEN = max_seq_len\n            \n        self.input_sequences = pad_sequences(self.input_sequences, maxlen=self.MAX_SEQ_LEN, padding=\"post\")\n        self.target_sequences = pad_sequences(self.target_sequences, maxlen=self.MAX_SEQ_LEN, padding=\"post\")\n        \n        print(\"1st input sequence: \", self.input_sequences[0])\n        print(\"1st target sequence: \", self.target_sequences[0])\n        \n        \n    def one_hot_encoding(self):\n        self.one_hot_targets = np.zeros((len(self.target_sequences), self.MAX_SEQ_LEN, self.num_words))\n        \n        for seq_idx, seq in enumerate(self.target_sequences):\n            for word_idx, word_id in enumerate(self.target_sequences[seq_idx]):\n                if word_id > 0:\n                    self.one_hot_targets[seq_idx, word_idx, word_id] = 1\n    \n    \n    def get_closest_word(self, word_vec):\n     \n        \n        max_dist = 9999999999\n        closest_word = \"NULL\"\n       \n        for word, vec in self.word2vec.items():\n        \n            dist = spatial.distance.cosine(word_vec, vec)\n            \n            if dist < max_dist:\n                max_dist = dist\n                closest_word = word\n        \n        return closest_word\n\nsg_obj = SequenceGenerator(input_lines, target_lines, max_seq_len=12, \n                           max_vocab_size=MAX_VOCAB_SIZE, embedding_dim=EMBEDDING_DIM)\n\n\nsg_obj.prepare_sequences()\nsg_obj.one_hot_encoding()\n\nassert '<sos>' in sg_obj.word2idx\nassert '<eos>' in sg_obj.word2idx","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:25.758978Z","iopub.execute_input":"2022-01-04T20:18:25.760771Z","iopub.status.idle":"2022-01-04T20:18:59.779886Z","shell.execute_reply.started":"2022-01-04T20:18:25.759166Z","shell.execute_reply":"2022-01-04T20:18:59.779064Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Create Seq2Seq model","metadata":{}},{"cell_type":"markdown","source":"## Create the Encoder","metadata":{}},{"cell_type":"code","source":"\nembedding = Embedding(\n    input_dim=sg_obj.num_words,\n    output_dim=sg_obj.EMBEDDING_DIM,\n    weights=[sg_obj.embeddings_matrix]\n)\n\n\nstate_h = Input(shape=(LATENT_DIM,))\nstate_c = Input(shape=(LATENT_DIM,))\nsequence_input = Input(shape=(sg_obj.MAX_SEQ_LEN,))\n\nembedding_ = embedding(sequence_input)\n\n\nlstm = LSTM(LATENT_DIM, return_state=True, return_sequences=True)\nx, h_, c_ = lstm(embedding_, initial_state=[state_h, state_c])\ndense = Dense(sg_obj.num_words, activation=\"softmax\")\noutput = dense(x)\n\nEncoder = Model([sequence_input, state_h, state_c], output)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:18:59.781122Z","iopub.execute_input":"2022-01-04T20:18:59.781583Z","iopub.status.idle":"2022-01-04T20:19:02.383386Z","shell.execute_reply.started":"2022-01-04T20:18:59.781534Z","shell.execute_reply":"2022-01-04T20:19:02.382531Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Create the Decoder","metadata":{}},{"cell_type":"code","source":"\ndeco_inp = Input(shape=(1,))\ndeco_embed = embedding(deco_inp)\ndeco_x, h, c = lstm(deco_embed, initial_state=[state_h, state_c])\ndeco_output = dense(deco_x)\nDecoder = Model([deco_inp, state_h, state_c], [deco_output, h, c])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:19:02.385026Z","iopub.execute_input":"2022-01-04T20:19:02.385316Z","iopub.status.idle":"2022-01-04T20:19:02.455005Z","shell.execute_reply.started":"2022-01-04T20:19:02.385270Z","shell.execute_reply":"2022-01-04T20:19:02.454283Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"Encoder.compile(\n    loss='categorical_crossentropy',\n    optimizer=Adam(lr=0.01),\n    metrics=['accuracy']\n)\n\ninitial_state = np.zeros((len(sg_obj.input_sequences), LATENT_DIM))\nhistory = Encoder.fit(\n    [sg_obj.input_sequences, initial_state, initial_state], \n    sg_obj.one_hot_targets, \n    batch_size=BATCH_SIZE, \n    epochs=EPOCHS, \n    validation_split=VALIDATION_SPLIT,\n    verbose=0\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:19:02.456592Z","iopub.execute_input":"2022-01-04T20:19:02.457006Z","iopub.status.idle":"2022-01-04T20:37:17.636493Z","shell.execute_reply.started":"2022-01-04T20:19:02.456837Z","shell.execute_reply":"2022-01-04T20:37:17.635563Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_context(sequences, query_word):\n\n    \n    assert query_word in sg_obj.word2idx\n    \n    \n    query_word_embed = sg_obj.word2vec.get(query_word, np.zeros(shape=(EMBEDDING_DIM)))\n    \n    if sequences == []:\n        return query_word_embed\n   \n    seq_embeddings = []\n    for seq in sequences:\n        \n        \n        zero_vector = np.zeros(shape=(EMBEDDING_DIM))\n        for word in seq:\n            zero_vector += sg_obj.word2vec.get(word, np.zeros(shape=(EMBEDDING_DIM)))\n            \n        seq_embeddings.append(zero_vector)\n    seq_embeddings = np.array(seq_embeddings)\n            \n    weights = []\n    for seq_embed in seq_embeddings:\n        \n        dist = spatial.distance.cosine(seq_embed, query_word_embed)\n        weights.append(np.array([dist]))\n        \n    \n    weights = np.array(weights/max(weights))\n        \n   \n    context = sum(weights * seq_embeddings)\n    \n    return context","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:37:17.638056Z","iopub.execute_input":"2022-01-04T20:37:17.638492Z","iopub.status.idle":"2022-01-04T20:37:17.648485Z","shell.execute_reply.started":"2022-01-04T20:37:17.638445Z","shell.execute_reply":"2022-01-04T20:37:17.647567Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_sample_line(context):\n \n    sos_token = np.array([[sg_obj.word2idx.get(\"<sos>\")]])\n   \n    h = np.array([context])    \n    c = np.zeros(shape=(1, LATENT_DIM))\n   \n    eos_token = sg_obj.word2idx['<eos>']\n    \n    output_sequence = []\n\n    for i in range(sg_obj.MAX_SEQ_LEN):\n        o, h, c = Decoder.predict([sos_token, h, c])\n        probs = o[0,0]\n        \n        if np.argmax(probs) ==0:\n            print(\"Something went wrong!!\")\n        \n        probs = np.nan_to_num(probs)\n        probs[0] = 0 \n        probs /= probs.sum()\n        selected_idx = np.random.choice(len(probs), p=probs)\n        if selected_idx == eos_token:\n            break\n        output_sequence.append(sg_obj.idx2word.get(selected_idx, \"Error <%d>\" % selected_idx))\n        \n        sos_token[0][0] = selected_idx\n    return output_sequence","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:37:17.650008Z","iopub.execute_input":"2022-01-04T20:37:17.650465Z","iopub.status.idle":"2022-01-04T20:37:17.662844Z","shell.execute_reply.started":"2022-01-04T20:37:17.650416Z","shell.execute_reply":"2022-01-04T20:37:17.661897Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\nquery_word = \"sun\"\npoem_lines = []\nsequences = []\nfor line_no in range(8):\n    context = get_context(sequences, query_word)\n    \n    try:\n        sequences.append(get_sample_line(context))\n    except:\n        pass\n    \n    poem_lines.append(\" \".join(sequences[-1]))\n    \nprint(\"\\n\\n\")\nprint(\"\\n\".join(poem_lines))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T20:37:17.664274Z","iopub.execute_input":"2022-01-04T20:37:17.664869Z","iopub.status.idle":"2022-01-04T20:37:17.901436Z","shell.execute_reply.started":"2022-01-04T20:37:17.664523Z","shell.execute_reply":"2022-01-04T20:37:17.899913Z"},"trusted":true},"execution_count":18,"outputs":[]}]}
